{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517367f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb181369",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "\n",
    "def rolling_mean(series, window): return series.rolling(window).mean()\n",
    "def rolling_std(series, window): return series.rolling(window).std()\n",
    "def rolling_sum(series, window): return series.rolling(window).sum()\n",
    "def ewma(series, span, min_periods): return series.ewm(span = span, min_periods = min_periods).mean()\n",
    "def get_value(df, idx, col): return df.iloc[idx][col]\n",
    "def minMaxScaling(column):\n",
    "    return (column - column.min())/(column.max()-column.min())\n",
    "\n",
    "#Moving Average\n",
    "def MA(df, n, feature = 'Close',):\n",
    "    MA = pd.Series(rolling_mean(df[feature], n), name = 'MA_' + str(n))\n",
    "    df['MA'+'_'+feature] = MA.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Exponential Moving Average\n",
    "def EMA(df, n, feature = 'Close',):\n",
    "    EMA = pd.Series(ewma(df[feature], span = n, min_periods = n - 1), name = 'EMA_' + str(n))\n",
    "    df['EMA'+'_'+feature] = EMA.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Momentum\n",
    "def MOM(df, n, feature = 'Close',):\n",
    "    M = pd.Series(df[feature].diff(n), name = 'Momentum_' + str(n))\n",
    "    df['MOM'+'_'+str(n)+'_'+feature] = M.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Rate of Change\n",
    "def ROC(df, n, feature = 'Close'):\n",
    "    M = df[feature].diff(n - 1)\n",
    "    N = df[feature].shift(n - 1)\n",
    "    ROC = pd.Series(M / N, name = 'ROC_' + str(n))\n",
    "    df['ROC'+'_'+str(n) +'_'+feature] = ROC.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Average True Range\n",
    "def ATR(df, n,):\n",
    "    i = 0\n",
    "    TR_l = [0]\n",
    "    while i < len(df) - 1:\n",
    "        TR = max(get_value(df, i + 1, 'High'), get_value(df, i, 'Close')) - min(get_value(df, i + 1, 'Low'), get_value(df, i, 'Close'))\n",
    "        TR_l.append(TR)\n",
    "        i = i + 1\n",
    "    TR_s = pd.Series(TR_l)\n",
    "    ATR = pd.Series(ewma(TR_s, span = n, min_periods = n), name = 'ATR_' + str(n))\n",
    "    #print(TR_l,TR_s, ATR)\n",
    "    df['ATR'] = ATR.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Bollinger Bands\n",
    "def BBANDS(df, n, feature = 'Close'):\n",
    "    MA = pd.Series(rolling_mean(df['Close'], n))\n",
    "    MSD = pd.Series(rolling_std(df['Close'], n))\n",
    "    b1 = 4 * MSD / MA\n",
    "    B1 = pd.Series(b1, name = 'BollingerB_' + str(n))\n",
    "    df['B1'+'_'+feature] = B1\n",
    "    b2 = (df['Close'] - MA + 2 * MSD) / (4 * MSD)\n",
    "    B2 = pd.Series(b2, name = 'Bollinger%b_' + str(n))\n",
    "    df['B2'+'_'+feature] = B2.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Pivot Points, Supports and Resistances\n",
    "def PPSR(df):\n",
    "    PP = pd.Series((df['High'] + df['Low'] + df['Close']) / 3)\n",
    "    R1 = pd.Series(2 * PP - df['Low'])\n",
    "    S1 = pd.Series(2 * PP - df['High'])\n",
    "    R2 = pd.Series(PP + df['High'] - df['Low'])\n",
    "    S2 = pd.Series(PP - df['High'] + df['Low'])\n",
    "    R3 = pd.Series(df['High'] + 2 * (PP - df['Low']))\n",
    "    S3 = pd.Series(df['Low'] - 2 * (df['High'] - PP))\n",
    "    psr = {'PP':PP, 'R1':R1, 'S1':S1, 'R2':R2, 'S2':S2, 'R3':R3, 'S3':S3}\n",
    "    PSR = pd.DataFrame(psr)\n",
    "    for col in PSR.columns:\n",
    "        df['PSR_' + col] = PSR[col].to_numpy()\n",
    "    return df\n",
    "\n",
    "#Stochastic oscillator %K\n",
    "def STOK(df):\n",
    "    SOk = pd.Series((df['Close'] - df['Low']) / (df['High'] - df['Low']), name = 'SO%k')\n",
    "    df['SOk'] = SOk.to_numpy()\n",
    "    return df\n",
    "\n",
    "# Stochastic Oscillator, EMA smoothing, nS = slowing (1 if no slowing)\n",
    "def STO(df,  nK, nD, nS=1):\n",
    "    SOk = pd.Series((df['Close'] - df['Low'].rolling(nK).min()) / (df['High'].rolling(nK).max() - df['Low'].rolling(nK).min()), name = 'SO%k'+str(nK))\n",
    "    SOd = pd.Series(SOk.ewm(ignore_na=False, span=nD, min_periods=nD-1, adjust=True).mean(), name = 'SO%d'+str(nD))\n",
    "    SOk = SOk.ewm(ignore_na=False, span=nS, min_periods=nS-1, adjust=True).mean()\n",
    "    SOd = SOd.ewm(ignore_na=False, span=nS, min_periods=nS-1, adjust=True).mean()\n",
    "    df['SOk'] = SOk.to_numpy()\n",
    "    df['SOd'] = SOd.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Trix\n",
    "def TRIX(df, n, feature = 'Close'):\n",
    "    EX1 = ewma(df[feature], span = n, min_periods = n - 1)\n",
    "    EX2 = ewma(EX1, span = n, min_periods = n - 1)\n",
    "    EX3 = ewma(EX2, span = n, min_periods = n - 1)\n",
    "    i = 0\n",
    "    ROC_l = [0]\n",
    "    while i + 1 <= len(df) - 1:\n",
    "        ROC = (EX3.iloc[i + 1] - EX3.iloc[i]) / EX3.iloc[i]\n",
    "        ROC_l.append(ROC)\n",
    "        i = i + 1\n",
    "    Trix = pd.Series(ROC_l, name = 'Trix_' + str(n))\n",
    "    df['Trix'+'_'+feature] = Trix.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Average Directional Movement Index\n",
    "def ADX(df, n, n_ADX):\n",
    "    i = 0\n",
    "    UpI = []\n",
    "    DoI = []\n",
    "    while i + 1 <= len(df) - 1:\n",
    "        UpMove = get_value(df, i + 1, 'High') - get_value(df, i, 'High')\n",
    "        DoMove = get_value(df, i, 'Low') - get_value(df, i + 1, 'Low')\n",
    "        if UpMove > DoMove and UpMove > 0:\n",
    "            UpD = UpMove\n",
    "        else: UpD = 0\n",
    "        UpI.append(UpD)\n",
    "        if DoMove > UpMove and DoMove > 0:\n",
    "            DoD = DoMove\n",
    "        else: DoD = 0\n",
    "        DoI.append(DoD)\n",
    "        i = i + 1\n",
    "    i = 0\n",
    "    TR_l = [0]\n",
    "    while i < len(df) - 1:\n",
    "        TR = max(get_value(df, i + 1, 'High'), get_value(df, i, 'Close')) - min(get_value(df, i + 1, 'Low'), get_value(df, i, 'Close'))\n",
    "        TR_l.append(TR)\n",
    "        i = i + 1\n",
    "    TR_s = pd.Series(TR_l)\n",
    "    ATR = pd.Series(ewma(TR_s, span = n, min_periods = n))\n",
    "    UpI = pd.Series(UpI)\n",
    "    DoI = pd.Series(DoI)\n",
    "    PosDI = pd.Series(ewma(UpI, span = n, min_periods = n - 1) / ATR)\n",
    "    NegDI = pd.Series(ewma(DoI, span = n, min_periods = n - 1) / ATR)\n",
    "    ADX = pd.Series(ewma(abs(PosDI - NegDI) / (PosDI + NegDI), span = n_ADX, min_periods = n_ADX - 1), name = 'ADX_' + str(n) + '_' + str(n_ADX))\n",
    "    df['ADX'] = ADX.to_numpy()\n",
    "    return df\n",
    "\n",
    "#MACD, MACD Signal and MACD difference\n",
    "def MACD(df, n_fast, n_slow, feature = 'Close'):\n",
    "    EMAfast = pd.Series(ewma(df[feature], span = n_fast, min_periods = n_slow - 1))\n",
    "    EMAslow = pd.Series(ewma(df[feature], span = n_slow, min_periods = n_slow - 1))\n",
    "    MACD = pd.Series(EMAfast - EMAslow, name = 'MACD_' + str(n_fast) + '_' + str(n_slow))\n",
    "    MACDsign = pd.Series(ewma(MACD, span = 9, min_periods = 8), name = 'MACDsign_' + str(n_fast) + '_' + str(n_slow))\n",
    "    MACDdiff = pd.Series(MACD - MACDsign, name = 'MACDdiff_' + str(n_fast) + '_' + str(n_slow))\n",
    "    df['MACD'+'_'+feature] = MACD.to_numpy()\n",
    "    df['MACDsign'+'_'+feature] = MACDsign.to_numpy()\n",
    "    df['MACDdiff'+'_'+feature] = MACDdiff.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Mass Index\n",
    "def MassI(df):\n",
    "    Range = df['High'] - df['Low']\n",
    "    EX1 = ewma(Range, span = 9, min_periods = 8)\n",
    "    EX2 = ewma(EX1, span = 9, min_periods = 8)\n",
    "    Mass = EX1 / EX2\n",
    "    MassI = pd.Series(rolling_sum(Mass, 25), name = 'Mass Index')\n",
    "    df['MassI'] = MassI.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Vortex Indicator: http://www.vortexindicator.com/VFX_VORTEX.PDF\n",
    "def Vortex(df, n):\n",
    "    i = 0\n",
    "    TR = [0]\n",
    "    while i < len(df) - 1:\n",
    "        Range = max(get_value(df, i + 1, 'High'), get_value(df, i, 'Close')) - min(get_value(df, i + 1, 'Low'), get_value(df, i, 'Close'))\n",
    "        TR.append(Range)\n",
    "        i = i + 1\n",
    "    i = 0\n",
    "    VM = [0]\n",
    "    while i < len(df) - 1:\n",
    "        Range = abs(get_value(df, i + 1, 'High') - get_value(df, i, 'Low')) - abs(get_value(df, i + 1, 'Low') - get_value(df, i, 'High'))\n",
    "        VM.append(Range)\n",
    "        i = i + 1\n",
    "    VI = pd.Series(rolling_sum(pd.Series(VM), n) / rolling_sum(pd.Series(TR), n), name = 'Vortex_' + str(n))\n",
    "    df['VI'] = VI.to_numpy()\n",
    "    return df\n",
    "\n",
    "#KST Oscillator\n",
    "def KST(df, r1, r2, r3, r4, n1, n2, n3, n4, feature = 'Close'):\n",
    "    M = df[feature].diff(r1 - 1)\n",
    "    N = df[feature].shift(r1 - 1)\n",
    "    ROC1 = M / N\n",
    "    M = df[feature].diff(r2 - 1)\n",
    "    N = df[feature].shift(r2 - 1)\n",
    "    ROC2 = M / N\n",
    "    M = df[feature].diff(r3 - 1)\n",
    "    N = df[feature].shift(r3 - 1)\n",
    "    ROC3 = M / N\n",
    "    M = df[feature].diff(r4 - 1)\n",
    "    N = df[feature].shift(r4 - 1)\n",
    "    ROC4 = M / N\n",
    "    KST = pd.Series(rolling_sum(ROC1, n1) + rolling_sum(ROC2, n2) * 2 + rolling_sum(ROC3, n3) * 3 + rolling_sum(ROC4, n4) * 4, name = 'KST_' + str(r1) + '_' + str(r2) + '_' + str(r3) + '_' + str(r4) + '_' + str(n1) + '_' + str(n2) + '_' + str(n3) + '_' + str(n4))\n",
    "    df['KST'+'_'+feature] = KST.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Relative Strength Index\n",
    "def RSI(df, n):\n",
    "    i = 0\n",
    "    UpI = [0]\n",
    "    DoI = [0]\n",
    "    while i + 1 <= len(df) - 1:\n",
    "        UpMove = get_value(df, i + 1, 'High') - get_value(df, i, 'High')\n",
    "        DoMove = get_value(df, i, 'Low') - get_value(df, i + 1, 'Low')\n",
    "        if UpMove > DoMove and UpMove > 0:\n",
    "            UpD = UpMove\n",
    "        else: UpD = 0\n",
    "        UpI.append(UpD)\n",
    "        if DoMove > UpMove and DoMove > 0:\n",
    "            DoD = DoMove\n",
    "        else: DoD = 0\n",
    "        DoI.append(DoD)\n",
    "        i = i + 1\n",
    "    UpI = pd.Series(UpI)\n",
    "    DoI = pd.Series(DoI)\n",
    "    PosDI = pd.Series(ewma(UpI, span = n, min_periods = n - 1))\n",
    "    NegDI = pd.Series(ewma(DoI, span = n, min_periods = n - 1))\n",
    "    RSI = pd.Series(PosDI / (PosDI + NegDI), name = 'RSI_' + str(n))\n",
    "    df['RSI'] = RSI.to_numpy()\n",
    "    return df\n",
    "\n",
    "#True Strength Index\n",
    "def TSI(df, r, s, feature = 'Close'):\n",
    "    M = pd.Series(df[feature].diff(1))\n",
    "    aM = abs(M)\n",
    "    EMA1 = pd.Series(ewma(M, span = r, min_periods = r - 1))\n",
    "    aEMA1 = pd.Series(ewma(aM, span = r, min_periods = r - 1))\n",
    "    EMA2 = pd.Series(ewma(EMA1, span = s, min_periods = s - 1))\n",
    "    aEMA2 = pd.Series(ewma(aEMA1, span = s, min_periods = s - 1))\n",
    "    TSI = pd.Series(EMA2 / aEMA2, name = 'TSI_' + str(r) + '_' + str(s))\n",
    "    df['TSI'+'_'+feature] = TSI.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Accumulation/Distribution\n",
    "def ACCDIST(df, n):\n",
    "    ad = (2 * df['Close'] - df['High'] - df['Low']) / (df['High'] - df['Low']) * df['Volume']\n",
    "    M = ad.diff(n - 1)\n",
    "    N = ad.shift(n - 1)\n",
    "    ROC = M / N\n",
    "    AD = pd.Series(ROC, name = 'Acc/Dist_ROC_' + str(n))\n",
    "    df['AD'] = AD.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Chaikin Oscillator\n",
    "def Chaikin(df):\n",
    "    ad = (2 * df['Close'] - df['High'] - df['Low']) / (df['High'] - df['Low']) * df['Volume']\n",
    "    Chaikin = pd.Series(ewma(ad, span = 3, min_periods = 2) - ewma(ad, span = 10, min_periods = 9), name = 'Chaikin')\n",
    "    df['Chaikin'] = Chaikin.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Money Flow Index and Ratio\n",
    "def MFI(df, n):\n",
    "    PP = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    i = 0\n",
    "    PosMF = [0]\n",
    "    while i < len(df) - 1:\n",
    "        if PP.iloc[i + 1] > PP.iloc[i]:\n",
    "            PosMF.append(PP.iloc[i + 1] * get_value(df, i + 1, 'Volume'))\n",
    "        else:\n",
    "            PosMF.append(0)\n",
    "        i = i + 1\n",
    "    PosMF = pd.Series(PosMF)\n",
    "    TotMF = PP * df['Volume']\n",
    "    #print(PosMF, TotMF)\n",
    "    MFR = pd.Series(PosMF.to_numpy() / TotMF.to_numpy())\n",
    "    MFI = pd.Series(rolling_mean(MFR, n), name = 'MFI_' + str(n))\n",
    "    df['MFI'] = MFI.to_numpy()\n",
    "    return df\n",
    "\n",
    "#On-balance Volume\n",
    "def OBV(df, n):\n",
    "    i = 0\n",
    "    OBV = [0]\n",
    "    while i < len(df) - 1:\n",
    "        if get_value(df, i + 1, 'Close') - get_value(df, i, 'Close') > 0:\n",
    "            OBV.append(get_value(df, i + 1, 'Volume'))\n",
    "        if get_value(df, i + 1, 'Close') - get_value(df, i, 'Close') == 0:\n",
    "            OBV.append(0)\n",
    "        if get_value(df, i + 1, 'Close') - get_value(df, i, 'Close') < 0:\n",
    "            OBV.append(-get_value(df, i + 1, 'Volume'))\n",
    "        i = i + 1\n",
    "    OBV = pd.Series(OBV)\n",
    "    OBV_ma = pd.Series(rolling_mean(OBV, n), name = 'OBV_' + str(n))\n",
    "    df['OBV_ma'] = OBV_ma.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Force Index\n",
    "def FORCE(df, n):\n",
    "    F = pd.Series(df['Close'].diff(n) * df['Volume'].diff(n), name = 'Force_' + str(n))\n",
    "    df['F'] = F.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Ease of Movement\n",
    "def EOM(df, n):\n",
    "    EoM = (df['High'].diff(1) + df['Low'].diff(1)) * (df['High'] - df['Low']) / (2 * df['Volume'])\n",
    "    Eom_ma = pd.Series(rolling_mean(EoM, n), name = 'EoM_' + str(n))\n",
    "    df['Eom_ma'] = Eom_ma.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Commodity Channel Index\n",
    "def CCI(df, n):\n",
    "    PP = (df['High'] + df['Low'] + df['Close']) / 3\n",
    "    CCI = pd.Series((PP - rolling_mean(PP, n)) / rolling_std(PP, n), name = 'CCI_' + str(n))\n",
    "    df['CCI'] = CCI.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Coppock Curve\n",
    "def COPP(df, n, feature = 'Close'):\n",
    "    M = df[feature].diff(int(n * 11 / 10) - 1)\n",
    "    N = df[feature].shift(int(n * 11 / 10) - 1)\n",
    "    ROC1 = M / N\n",
    "    M = df[feature].diff(int(n * 14 / 10) - 1)\n",
    "    N = df[feature].shift(int(n * 14 / 10) - 1)\n",
    "    ROC2 = M / N\n",
    "    Copp = pd.Series(ewma(ROC1 + ROC2, span = n, min_periods = n), name = 'Copp_' + str(n))\n",
    "    df['Copp'+'_'+feature] = Copp.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Keltner Channel\n",
    "def KELCH(df, n):\n",
    "    KelChM = pd.Series(rolling_mean((df['High'] + df['Low'] + df['Close']) / 3, n), name = 'KelChM_' + str(n))\n",
    "    KelChU = pd.Series(rolling_mean((4 * df['High'] - 2 * df['Low'] + df['Close']) / 3, n), name = 'KelChU_' + str(n))\n",
    "    KelChD = pd.Series(rolling_mean((-2 * df['High'] + 4 * df['Low'] + df['Close']) / 3, n), name = 'KelChD_' + str(n))\n",
    "    df['KelChM'] = KelChM.to_numpy()\n",
    "    df['KelChU'] = KelChU.to_numpy()\n",
    "    df['KelChD'] = KelChD.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Ultimate Oscillator\n",
    "def ULTOSC(df):\n",
    "    i = 0\n",
    "    TR_l = [0]\n",
    "    BP_l = [0]\n",
    "    while i < len(df) - 1:\n",
    "        TR = max(get_value(df, i + 1, 'High'), get_value(df, i, 'Close')) - min(get_value(df, i + 1, 'Low'), get_value(df, i, 'Close'))\n",
    "        TR_l.append(TR)\n",
    "        BP = get_value(df, i + 1, 'Close') - min(get_value(df, i + 1, 'Low'), get_value(df, i, 'Close'))\n",
    "        BP_l.append(BP)\n",
    "        i = i + 1\n",
    "    UltO = pd.Series((4 * rolling_sum(pd.Series(BP_l), 7) / rolling_sum(pd.Series(TR_l), 7)) + (2 * rolling_sum(pd.Series(BP_l), 14) / rolling_sum(pd.Series(TR_l), 14)) + (rolling_sum(pd.Series(BP_l), 28) / rolling_sum(pd.Series(TR_l), 28)), name = 'Ultimate_Osc')\n",
    "    df['UltO'] = UltO.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Donchian Channel\n",
    "def DONCH(df, n):\n",
    "    i = 0\n",
    "    DC_l = []\n",
    "    while i < n - 1:\n",
    "        DC_l.append(0)\n",
    "        i = i + 1\n",
    "    i = 0\n",
    "    while i + n - 1 <= (df.shape[0]) - 1:\n",
    "        DC = max(df['High'].iloc[i:i + n - 1]) - min(df['Low'].iloc[i:i + n - 1])\n",
    "        DC_l.append(DC)\n",
    "        i = i + 1\n",
    "    DonCh = pd.Series(DC_l, name = 'Donchian_' + str(n))\n",
    "    #print(DonCh)\n",
    "    DonCh = DonCh.shift(n - 1)\n",
    "    #print(DonCh)\n",
    "    df['DonCh'] = DonCh.to_numpy()\n",
    "    return df\n",
    "\n",
    "#Standard Deviation\n",
    "def STDDEV(df, n, feature = 'Close'):\n",
    "    std_dev = pd.Series(rolling_std(df[feature], n), name = 'STD_' + str(n))\n",
    "    df['std_dev'+'_'+feature] = std_dev.to_numpy()\n",
    "    return df\n",
    "\n",
    "#volitility functions\n",
    "\n",
    "def realized1(close, N=240):\n",
    "    rt = list(np.log(C_t / C_t_1) for C_t, C_t_1 in zip(close[1:], close[:-1]))\n",
    "    rt_mean = sum(rt) / len(rt)\n",
    "    return np.sqrt(sum((r_i - rt_mean) ** 2 for r_i in rt) * N / (len(rt) - 1))\n",
    "\n",
    "def parkinson1(high, low, N=240):\n",
    "    sum_hl = sum(np.log(H_t / L_t) ** 2 for H_t, L_t in zip(high, low))\n",
    "    result = np.sqrt(sum_hl * N / (4 * len(high) *np.log(2))) \n",
    "    #print(result)\n",
    "    return result\n",
    "\n",
    "def garman_klass1(open, high, low, close, N=240):\n",
    "    sum_hl = sum(np.log(H_t / L_t) ** 2 for H_t, L_t in zip(high, low)) / 2\n",
    "    sum_co = sum(np.log(C_t / O_t) ** 2 for C_t, O_t in zip(close, open)) * (2 * np.log(2) - 1)\n",
    "    return np.sqrt((sum_hl - sum_co) * N / len(close))\n",
    "\n",
    "def roger_satchell1(open1, high, low, close, N=240):\n",
    "    sum_ohlc = sum(np.log(H_t / C_t) * np.log(H_t / O_t) + np.log(L_t / C_t) * np.log(L_t / O_t) for O_t, H_t, L_t, C_t in zip(open1, high, low, close))\n",
    "    #print(close, len(close))\n",
    "    return np.sqrt(sum_ohlc * N / len(close))\n",
    "\n",
    "def yang_zhang1(open, high, low, close, N=240):\n",
    "    oc = list(np.log(O_t / C_t_1) for O_t, C_t_1 in zip(open[1:], close[:-1]))\n",
    "    n = len(oc)\n",
    "    oc_mean = sum(oc) / n\n",
    "    oc_var = sum((oc_i - oc_mean) ** 2 for oc_i in oc) * N / (n - 1)   \n",
    "    co = list(np.log(C_t / O_t) for O_t, C_t in zip(open[1:], close[1:]))\n",
    "    co_mean = sum(co) / n\n",
    "    co_var = sum((co_i - co_mean) ** 2 for co_i in co) * N / (n - 1)    \n",
    "    rs_var = (roger_satchell1(open, high, low, close)) ** 2    \n",
    "    k = 0.34 / (1.34 + (n +1) / (n - 1))    \n",
    "    return np.sqrt(oc_var + k * co_var + (1-k) * rs_var)\n",
    "\n",
    "def garkla_yangzh1(open, high, low, close, N=240):\n",
    "    sum_oc_1 = sum(np.log(O_t / C_t_1) ** 2 for O_t, C_t_1 in zip(open, close))\n",
    "    sum_hl = sum(np.log(H_t / L_t) ** 2 for H_t, L_t in zip(high, low)) / 2\n",
    "    sum_co = sum(np.log(C_t / O_t) ** 2 for C_t, O_t in zip(close, open)) * (2 * np.log(2) - 1)\n",
    "    return np.sqrt((sum_oc_1 + sum_hl - sum_co) * N / (len(close) - 1))\n",
    "\n",
    "def realized(df, n = 60, feature = 'Close'):\n",
    "    df['realized' + '_'+str(n)+'_'+feature] = df[feature].rolling(n).apply(realized1)\n",
    "    return df\n",
    "    \n",
    "def parkinson(df, n = 60):\n",
    "    #print(df.rolling(60))\n",
    "    df['parkinson'] = df.rolling(int(n)).apply(lambda x: parkinson1(df.loc[x.index, 'High'], df.loc[x.index, 'Low']))['Low']\n",
    "    return df\n",
    "\n",
    "def garman_klass(df, n = 60):\n",
    "    df['garman_klass'] = df.rolling(n).apply(lambda x: garman_klass1(df.loc[x.index, 'Open'], df.loc[x.index, 'High'], df.loc[x.index, 'Low'], df.loc[x.index, 'Close']))['Low']\n",
    "    return df\n",
    "\n",
    "def roger_satchell(df, n = 60):\n",
    "    df['roger_satchell'] = df.rolling(n).apply(lambda x: roger_satchell1(df.loc[x.index, 'Open'], df.loc[x.index, 'High'], df.loc[x.index, 'Low'], df.loc[x.index, 'Close']))['Low']\n",
    "    return df\n",
    "\n",
    "def yang_zang(df, n = 60):\n",
    "    a= df.rolling(n).apply(lambda x: yang_zhang1(df.loc[x.index, 'Open'], df.loc[x.index, 'High'], df.loc[x.index, 'Low'], df.loc[x.index, 'Close']))\n",
    "    #print(a)\n",
    "    df['yang_zang'] = a\n",
    "    return df\n",
    "\n",
    "def garkla_yangzh(df, n = 60):\n",
    "    df['garkla_yangzh'] = df.rolling(n).apply(lambda x: garkla_yangzh1(df.loc[x.index, 'Open'], df.loc[x.index, 'High'], df.loc[x.index, 'Low'], df.loc[x.index, 'Close']))\n",
    "    #print(df)\n",
    "    return df\n",
    "\n",
    "def volumeFeatures(df, n=60):\n",
    "    indexes = df.index\n",
    "    df['parkinson']      = 0\n",
    "    df['garman_klass']   = 0\n",
    "    df['roger_satchell'] = 0\n",
    "    df['yang_zang']      = 0\n",
    "    df['garkla_yangzh']  = 0\n",
    "    for i in range(df.shape[0]-60):\n",
    "        j = i + n -1\n",
    "        high = df.loc[indexes[i]:indexes[j], 'High']\n",
    "        low = df.loc[indexes[i]:indexes[j], 'Low']\n",
    "        open1 = df.loc[indexes[i]:indexes[j], 'Open']\n",
    "        close = df.loc[indexes[i]:indexes[j], 'Close']\n",
    "        df.loc[indexes[j+1], 'parkinson']      = parkinson1(high, low)\n",
    "        df.loc[indexes[j+1], 'garman_klass']   = garman_klass1(open1, high, low, close)\n",
    "        df.loc[indexes[j+1], 'roger_satchell'] = roger_satchell1(open1, high, low, close)\n",
    "        df.loc[indexes[j+1], 'yang_zang']      = yang_zhang1(open1, high, low, close)\n",
    "        df.loc[indexes[j+1], 'garkla_yangzh']  = garkla_yangzh1(open1, high, low, close)\n",
    "    return df\n",
    "\n",
    "def addTimeFeature(biggestBatch):\n",
    "    indexes = biggestBatch.index\n",
    "    ## add time variables\n",
    "    biggestBatch['Weekday'] = 0\n",
    "    biggestBatch['Hour'] = 0\n",
    "    biggestBatch['Month'] = 0\n",
    "    biggestBatch['Minute'] = 0\n",
    "    biggestBatch['Year'] = 0\n",
    "\n",
    "    for i in range(biggestBatch.shape[0]):\n",
    "        info = datetime.fromtimestamp(biggestBatch.timestamp.iloc[i])\n",
    "        biggestBatch.loc[indexes[i], 'Weekday'] = info.weekday()\n",
    "        biggestBatch.loc[indexes[i], 'Hour'] = info.hour\n",
    "        biggestBatch.loc[indexes[i], 'Month'] = info.month\n",
    "        biggestBatch.loc[indexes[i], 'Minute'] = info.minute\n",
    "        biggestBatch.loc[indexes[i], 'Year'] = info.year\n",
    "    return biggestBatch\n",
    "\n",
    "def upper_shadow(df): return df['High'] - np.maximum(df['Close'], df['Open'])\n",
    "def lower_shadow(df): return np.minimum(df['Close'], df['Open']) - df['Low']\n",
    "\n",
    "def normalFeatures(df_feat):\n",
    "    df_feat['spread'] = df_feat['High'] - df_feat['Low']\n",
    "    df_feat['mean_trade'] = df_feat['Volume']/df_feat['Count']\n",
    "    df_feat['log_price_change'] = np.log(df_feat['Close']/df_feat['Open'])\n",
    "    df_feat['upper_Shadow'] = upper_shadow(df_feat)\n",
    "    df_feat['lower_Shadow'] = lower_shadow(df_feat)\n",
    "    df_feat[\"high_div_low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n",
    "    df_feat['trade'] = df_feat['Close'] - df_feat['Open']\n",
    "    df_feat['gtrade'] = df_feat['trade'] / df_feat['Count']\n",
    "    df_feat['shadow1'] = df_feat['trade'] / df_feat['Volume']\n",
    "    df_feat['shadow3'] = df_feat['upper_Shadow'] / df_feat['Volume']\n",
    "    df_feat['shadow5'] = df_feat['lower_Shadow'] / df_feat['Volume']\n",
    "    df_feat['diff1'] = df_feat['Volume'] - df_feat['Count']\n",
    "    df_feat['mean1'] = (df_feat['shadow5'] + df_feat['shadow3']) / 2\n",
    "    df_feat['mean2'] = (df_feat['shadow1'] + df_feat['Volume']) / 2\n",
    "    df_feat['mean3'] = (df_feat['trade'] + df_feat['gtrade']) / 2\n",
    "    df_feat['mean4'] = (df_feat['diff1'] + df_feat['upper_Shadow']) / 2\n",
    "    df_feat['mean5'] = (df_feat['diff1'] + df_feat['lower_Shadow']) / 2\n",
    "    df_feat['UPS'] = (df_feat['High'] - np.maximum(df_feat['Close'], df_feat['Open']))\n",
    "    df_feat['UPS'] = df_feat['UPS']\n",
    "    df_feat['LOS'] = (np.minimum(df_feat['Close'], df_feat['Open']) - df_feat['Low'])\n",
    "    df_feat['LOS'] = df_feat['LOS']\n",
    "    df_feat['RNG'] = ((df_feat['High'] - df_feat['Low']) / df_feat['VWAP'])\n",
    "    df_feat['RNG'] = df_feat['RNG']\n",
    "    df_feat['MOV'] = ((df_feat['Close'] - df_feat['Open']) / df_feat['VWAP'])\n",
    "    df_feat['MOV'] = df_feat['MOV']\n",
    "    df_feat['CLS'] = ((df_feat['Close'] - df_feat['VWAP']) / df_feat['VWAP'])\n",
    "    df_feat['CLS'] = df_feat['CLS']\n",
    "    df_feat['LOGVOL'] = np.log(1. + df_feat['Volume'])\n",
    "    df_feat['LOGVOL'] = df_feat['LOGVOL']\n",
    "    df_feat['LOGCNT'] = np.log(1. + df_feat['Count'])\n",
    "    df_feat['LOGCNT'] = df_feat['LOGCNT']\n",
    "    df_feat[\"Close/Open\"] = df_feat[\"Close\"] / df_feat[\"Open\"]\n",
    "    df_feat[\"Close-Open\"] = df_feat[\"Close\"] - df_feat[\"Open\"]\n",
    "    df_feat[\"High-Low\"] = df_feat[\"High\"] - df_feat[\"Low\"]\n",
    "    df_feat[\"High/Low\"] = df_feat[\"High\"] / df_feat[\"Low\"]\n",
    "    #if row: df_feat['Mean'] = df_feat[['Open', 'High', 'Low', 'Close']].mean()\n",
    "    df_feat['Mean'] = np.mean(df_feat[['Open', 'High', 'Low', 'Close']].mean().to_numpy())\n",
    "    #else: df_feat['Mean'] = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis = 1)\n",
    "    df_feat[\"High/Mean\"] = df_feat[\"High\"] / df_feat[\"Mean\"]\n",
    "    df_feat[\"Low/Mean\"] = df_feat[\"Low\"] / df_feat[\"Mean\"]\n",
    "    df_feat[\"Volume/Count\"] = df_feat[\"Volume\"] / (df_feat[\"Count\"] + 1)\n",
    "    mean_price = df_feat[['Open', 'High', 'Low', 'Close']].mean(axis=1)\n",
    "    median_price = df_feat[['Open', 'High', 'Low', 'Close']].median(axis=1)\n",
    "    df_feat['high2mean'] = df_feat['High'] / mean_price\n",
    "    df_feat['low2mean'] = df_feat['Low'] / mean_price\n",
    "    df_feat['high2median'] = df_feat['High'] / median_price\n",
    "    df_feat['low2median'] = df_feat['Low'] / median_price\n",
    "    df_feat['volume2count'] = df_feat['Volume'] / (df_feat['Count'] + 1)\n",
    "    return df_feat\n",
    "\n",
    "def normalizeFeatures(df):\n",
    "    for feature in df.columns.drop(['Target','Asset_ID', 'timestamp', 'batch', 'batchSize']):\n",
    "        df[feature] = minMaxScaling(df[feature])\n",
    "    return df\n",
    "\n",
    "def neutralizeFeatures(df, proportion=1.0):\n",
    "    by = df['Target']\n",
    "    for feature in df.columns.drop(['Target','Asset_ID', 'timestamp', 'batch', 'batchSize']):\n",
    "        series = df[feature]\n",
    "\n",
    "        scores = np.nan_to_num(series.values).reshape(-1, 1)\n",
    "        exposures = np.nan_to_num(by.values).reshape(-1, 1)\n",
    "        exposures = np.hstack((exposures, np.array([np.mean(np.nan_to_num(series.values))] * len(exposures)).reshape(-1, 1)))\n",
    "        correction = proportion * (exposures.dot(np.linalg.lstsq(exposures, scores)[0]))\n",
    "        corrected_scores = scores - correction\n",
    "        df['neutralized'+'_'+feature] = pd.Series(corrected_scores.ravel(), index=series.index)\n",
    "        if feature == 'Target':\n",
    "            break\n",
    "    return df\n",
    "\n",
    "def addFeaturesToDataSet(df):\n",
    "    n = 15\n",
    "    nK = 5\n",
    "    nD = 7\n",
    "    n_fast = 5\n",
    "    n_slow = 10\n",
    "    n_ADX = 10\n",
    "    r = 3\n",
    "    s = 7\n",
    "    n_longTerm = 10000\n",
    "    n_shortTerm = 100\n",
    "    n_intermediate = 1000\n",
    "    \n",
    "    original_features = df.columns.drop(['Target','Asset_ID', 'timestamp', 'batch', 'batchSize'])\n",
    "\n",
    "    df = volumeFeatures(df, 60)\n",
    "    df = ATR(df, n)\n",
    "    df = PPSR(df)\n",
    "    df = STOK(df)\n",
    "    df = STO(df,  nK, nD, nS=1)\n",
    "    df = ADX(df, n, n_ADX)\n",
    "    df = MassI(df)\n",
    "    df = Vortex(df, n)\n",
    "    df = RSI(df, n)\n",
    "    df = ACCDIST(df, n)\n",
    "    df = Chaikin(df)\n",
    "    df = MFI(df, n)\n",
    "    df = OBV(df, n)\n",
    "    df = FORCE(df, n)\n",
    "    df = EOM(df, n)\n",
    "    df = CCI(df, n)\n",
    "    df = KELCH(df, n)\n",
    "    df = ULTOSC(df)\n",
    "    df = DONCH(df, n)\n",
    "    \n",
    "    df = normalFeatures(df)\n",
    "    print(\"finished adding normal features\")\n",
    "    #feature independent\n",
    "    for feature in original_features:\n",
    "        df[feature + '_RollingMean_' + str(n)] = rolling_mean(df[feature], n)\n",
    "        df[feature + '_RollingMean_' + str(n)] = rolling_sum(df[feature], n)\n",
    "        df = MA(df, n, feature)\n",
    "        df = EMA(df, n, feature)\n",
    "        df = MOM(df, n, feature)\n",
    "        df = ROC(df, n, feature)\n",
    "        df = BBANDS(df, n, feature)\n",
    "        df = TRIX(df, n, feature)\n",
    "        df = MACD(df, n_fast, n_slow, feature)\n",
    "        df = KST(df, 10, 15, 20, 30, 10, 10, 10, 15, feature)\n",
    "        df = TSI(df, r, s, feature)\n",
    "        df = COPP(df, n, feature)\n",
    "        df = STDDEV(df, n, feature)\n",
    "        \n",
    "        df = ROC(df, n_longTerm, feature)\n",
    "        df = ROC(df, n_shortTerm, feature)\n",
    "        df = ROC(df, n_intermediate, feature)\n",
    "        \n",
    "        df = realized(df, 60, feature)\n",
    "    print(\"finished adding feature independent features\")\n",
    "    for feature in df.columns.drop(['Target','Asset_ID', 'timestamp']):\n",
    "        df = ROC(df, 2, feature)\n",
    "        df = ROC(df, 5, feature)\n",
    "        df = ROC(df, 20, feature)\n",
    "        \n",
    "    df = addTimeFeature(df)\n",
    "    \n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b05c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_hdf('03_non_padded_with_batch_info.h5')\n",
    "batchSize = max(np.unique(data.batchSize))\n",
    "batch = data.loc[data.batchSize == batchSize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767e729d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_hdf('03_non_padded_with_batch_info.h5', key = 'df', mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe20499",
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(data.batchSize)\n",
    "batch = data.loc[data.batchSize == 100]\n",
    "batch2 = batch.loc[batch.Asset_ID == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b71ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_feat = addFeaturesToDataSet(batch2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eeaee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processAllAssets(df):\n",
    "    cache = []\n",
    "    for i in np.unique(df.Asset_ID):\n",
    "        batch = df.loc[df.Asset_ID == i]\n",
    "        cache.append(.to_numpy())\n",
    "    return pd.concat(cache)\n",
    "\n",
    "def processBatch(df, size):\n",
    "    batch = data.loc[data.batchSize == size]\n",
    "    return processAllAssets(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ba546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "processAllAssets(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(data.batchSize)\n",
    "\n",
    "biggestBatch = processBatch(data, 28276)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbce345e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data.loc[data.batchSize == 1019]#28276]\n",
    "batch3 = batch.loc[batch.Asset_ID == 3]\n",
    "a = addFeaturesToDataSet(batch3)\n",
    "\n",
    "print(a.shape)\n",
    "for feature in a.columns.values:\n",
    "    print(feature, a[feature].isnull().values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d2260b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del batch, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31609e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(biggestBatch.shape)\n",
    "for feature in biggestBatch.columns.values:\n",
    "    print(feature, biggestBatch[feature].isnull().values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418a9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggestBatch.to_hdf('05_biggestBatch_features.h5', key = 'df', mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8d41ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "info = datetime.fromtimestamp(biggestBatch.timestamp.iloc[60*24*i])\n",
    "print(info)\n",
    "info.year\n",
    "info.hour\n",
    "info.minute\n",
    "info.month\n",
    "info.weekday()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7671391d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def addTimeFeature(biggestBatch):\n",
    "    indexes = biggestBatch.index\n",
    "    ## add time variables\n",
    "    biggestBatch['Weekday'] = 0\n",
    "    biggestBatch['Hour'] = 0\n",
    "    biggestBatch['Month'] = 0\n",
    "    biggestBatch['Minute'] = 0\n",
    "    biggestBatch['Year'] = 0\n",
    "\n",
    "    for i in range(biggestBatch.shape[0]):\n",
    "        info = datetime.fromtimestamp(biggestBatch.timestamp.iloc[i])\n",
    "        biggestBatch.loc[indexes[i], 'Weekday'] = info.weekday()\n",
    "        biggestBatch.loc[indexes[i], 'Hour'] = info.hour\n",
    "        biggestBatch.loc[indexes[i], 'Month'] = info.month\n",
    "        biggestBatch.loc[indexes[i], 'Minute'] = info.minute\n",
    "        biggestBatch.loc[indexes[i], 'Year'] = info.year\n",
    "    return biggestBatch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dee8a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "addTimeFeature(biggestBatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deda80ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggestBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea15e88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggestBatch = biggestBatch.drop([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8dcfde0",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggestBatch = biggestBatch.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0aa9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggestBatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf4b31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "biggestBatch.to_hdf('05_biggestBatch_features_time.h5', key = 'df', mode = 'w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e3f0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch =  biggestBatch.loc[biggestBatch.Asset_ID == 0]\n",
    "batch.drop(['Target','Asset_ID', 'timestamp', 'batch', 'batchSize'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e1e5f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "assets = np.unique(biggestBatch.Asset_ID)\n",
    "cache = []\n",
    "for asset in assets:\n",
    "    batch = biggestBatch.loc[biggestBatch.Asset_ID == asset]\n",
    "    #print(batch)\n",
    "    X_train = batch.drop(['Target','Asset_ID', 'timestamp', 'batch', 'batchSize'], axis = 1).astype(np.float32)\n",
    "    Y_train = batch.Target.astype(np.float32)\n",
    "    print(np.sum(X_train.isnull().sum()), Y_train.isnull().sum())\n",
    "    \n",
    "    clf = RandomForestRegressor()\n",
    "    clf.fit(X_train, Y_train)\n",
    "    \n",
    "    y = clf.feature_importances_\n",
    "    x = np.linspace(0, len(y)-1, len(y))\n",
    "    fig=plt.figure()\n",
    "    plt.plot(x,y)\n",
    "    \n",
    "    cache.append(clf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
